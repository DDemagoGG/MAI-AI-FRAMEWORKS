{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee71b4d",
   "metadata": {},
   "source": [
    "# Лабораторная работа №2 \"Проведение исследований с логистической и линейной регрессией\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617fcef",
   "metadata": {},
   "source": [
    "### Ход работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27489ecd",
   "metadata": {},
   "source": [
    "Импортируем библиотеки перед работой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b84a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Ridge, Lasso\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, mean_absolute_error, r2_score, log_loss\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.utils.extmath import softmax\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a23ce3",
   "metadata": {},
   "source": [
    "##### Создание бейзлайна для модели классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb265045",
   "metadata": {},
   "source": [
    "Проведём те же манипуляции, что и ранне: выгрузим датасет и минимально его обработаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec22ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_base_df = pd.read_csv(\"../classification.csv\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "c_base_df = c_base_df.drop(columns=['instance_id', 'obtained_date', 'track_name', 'artist_name'])\n",
    "\n",
    "c_base_df.drop_duplicates()\n",
    "\n",
    "c_base_df['tempo'] = pd.to_numeric(c_base_df['tempo'], errors='coerce')\n",
    "\n",
    "le = LabelEncoder()\n",
    "c_base_df['mode'] = le.fit_transform(c_base_df['mode'])\n",
    "c_base_df['music_genre'] = le.fit_transform(c_base_df['music_genre'])\n",
    "c_base_df['key'] = le.fit_transform(c_base_df['key'])\n",
    "\n",
    "median_tempo = c_base_df['tempo'].median()\n",
    "c_base_df['tempo'] = c_base_df['tempo'].fillna(median_tempo)\n",
    "\n",
    "X_c_base = c_base_df.drop(columns=[\"music_genre\"])\n",
    "y_c_base = c_base_df[\"music_genre\"]\n",
    "\n",
    "X_c_base_train, X_c_base_test, y_c_base_train, y_c_base_test = train_test_split(\n",
    "    X_c_base,\n",
    "    y_c_base,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_c_base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef90f0d7",
   "metadata": {},
   "source": [
    "Теперь обучим модель из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651c97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3162\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model.fit(X_c_base_train, y_c_base_train)\n",
    "\n",
    "y_pred = model.predict(X_c_base_test)\n",
    "\n",
    "acc = accuracy_score(y_c_base_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9be55",
   "metadata": {},
   "source": [
    "##### Улучшение бейзлайна для модели классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b4f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.20      0.19      1000\n",
      "           1       0.49      0.58      0.53      1000\n",
      "           2       0.32      0.17      0.22      1000\n",
      "           3       0.67      0.76      0.71      1000\n",
      "           4       0.18      0.09      0.12      1000\n",
      "           5       0.23      0.31      0.27      1000\n",
      "           6       0.20      0.22      0.21      1000\n",
      "           7       0.31      0.33      0.32      1000\n",
      "           8       0.28      0.23      0.25      1000\n",
      "           9       0.22      0.28      0.25      1000\n",
      "\n",
      "    accuracy                           0.32     10000\n",
      "   macro avg       0.31      0.32      0.31     10000\n",
      "weighted avg       0.31      0.32      0.31     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_c_base_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796ebfd",
   "metadata": {},
   "source": [
    "Это явно лучше, чем KNN. Теперь снова применим техники из прошлой ЛР и улучшим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed2c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = pd.read_csv(\"../classification.csv\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "c_df = c_df.drop(columns=['instance_id', 'obtained_date', 'track_name', 'artist_name'])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "c_df[['pc1', 'pc2']] = pca.fit_transform(c_df[['loudness', 'acousticness', 'energy']])\n",
    "c_df = c_df.drop(columns=['loudness', 'acousticness', 'energy'])\n",
    "\n",
    "c_df['tempo'] = pd.to_numeric(c_df['tempo'], errors='coerce')\n",
    "\n",
    "le = LabelEncoder()\n",
    "c_df['music_genre'] = le.fit_transform(c_df['music_genre'])\n",
    "c_df['mode'] = le.fit_transform(c_df['mode'])\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_key = ohe.fit_transform(c_df[['key']])\n",
    "encoded_df_key = pd.DataFrame(encoded_key, columns=ohe.get_feature_names_out(['key']))\n",
    "c_df = c_df.drop(columns=['key']).reset_index(drop=True)\n",
    "c_df = pd.concat([c_df, encoded_df_key], axis=1)\n",
    "\n",
    "c_df['duration_ms'] = c_df['duration_ms'].replace(-1, np.nan)\n",
    "\n",
    "c_df['instrumental_flag'] = (c_df['instrumentalness'] > 0.05).astype(int)\n",
    "c_df = c_df.drop(columns=['instrumentalness'])\n",
    "\n",
    "c_df['undefined_tempo'] = c_df['tempo'].isna().astype(int)\n",
    "\n",
    "median_tempo = c_df['tempo'].median()\n",
    "c_df['tempo'] = c_df['tempo'].fillna(median_tempo)\n",
    "\n",
    "median_duration = c_df['duration_ms'].median()\n",
    "c_df['duration_ms'] = c_df['duration_ms'].fillna(median_duration)\n",
    "\n",
    "float_features = [\n",
    "    'popularity', 'danceability', 'duration_ms',\n",
    "    'liveness', 'speechiness', 'tempo',\n",
    "    'valence', 'pc1', 'pc2'\n",
    "]\n",
    "\n",
    "other_features = [\n",
    "    'mode', 'instrumental_flag', 'undefined_tempo'\n",
    "] + list(encoded_df_key.columns) \n",
    "\n",
    "X_c = c_df[float_features + other_features]\n",
    "y_c = c_df['music_genre']\n",
    "\n",
    "X_c_train, X_c_test, y_c_train, y_c_test = train_test_split(\n",
    "    X_c, y_c, test_size=0.2, stratify=y_c, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e9290",
   "metadata": {},
   "source": [
    "теперь перейдём к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab036fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best parameters: {'logreg__C': 0.1, 'logreg__class_weight': None, 'logreg__penalty': 'l1', 'logreg__solver': 'saga'}\n",
      "Score: 0.5282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.28      0.32      1000\n",
      "           1       0.62      0.63      0.63      1000\n",
      "           2       0.50      0.43      0.47      1000\n",
      "           3       0.76      0.79      0.78      1000\n",
      "           4       0.46      0.58      0.51      1000\n",
      "           5       0.56      0.60      0.58      1000\n",
      "           6       0.48      0.52      0.50      1000\n",
      "           7       0.49      0.42      0.45      1000\n",
      "           8       0.47      0.38      0.42      1000\n",
      "           9       0.51      0.65      0.57      1000\n",
      "\n",
      "    accuracy                           0.53     10000\n",
      "   macro avg       0.52      0.53      0.52     10000\n",
      "weighted avg       0.52      0.53      0.52     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('float_scaling', StandardScaler(), float_features),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('logreg', LogisticRegression(max_iter=5000, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__class_weight': [None, 'balanced'],\n",
    "    'logreg__solver': ['saga', 'liblinear']\n",
    "}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_c_train, y_c_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "y_c_pred = best_model.predict(X_c_test)\n",
    "print(\"Score:\", accuracy_score(y_c_test, y_c_pred))\n",
    "\n",
    "print(classification_report(y_c_test, y_c_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c100a19",
   "metadata": {},
   "source": [
    "##### Создание бейзлайна для модели регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0b77b",
   "metadata": {},
   "source": [
    "Сделаем всё то же, что и ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d013fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_base_df = pd.read_csv(\"../regression.csv\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "r_base_df['Date'] = pd.to_datetime(r_base_df['Date'], dayfirst=True)\n",
    "\n",
    "r_base_df[\"Year\"] = r_base_df[\"Date\"].dt.year\n",
    "r_base_df[\"Month\"] = r_base_df[\"Date\"].dt.month\n",
    "r_base_df[\"Day\"] = r_base_df[\"Date\"].dt.day\n",
    "\n",
    "r_base_df = r_base_df.drop(columns=['Date'])\n",
    "\n",
    "per_store_count = r_base_df.groupby('Store').size().iloc[0]\n",
    "k = max(1, int(np.round(0.8 * per_store_count))) \n",
    "store_counts = r_base_df['Store'].nunique()\n",
    "\n",
    "train = r_base_df.iloc[: store_counts * k]\n",
    "test = r_base_df.iloc[store_counts * k :]\n",
    "\n",
    "X_r_base_train = train.drop(columns=['Weekly_Sales'])\n",
    "X_r_base_test = test.drop(columns=['Weekly_Sales'])\n",
    "y_r_base_train = train['Weekly_Sales']\n",
    "y_r_base_test = test['Weekly_Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ccde6",
   "metadata": {},
   "source": [
    "И теперь обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80949072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 423673.5471083975\n",
      "RMSE: 520176.70459250984\n",
      "R2: 0.1636445664651608\n"
     ]
    }
   ],
   "source": [
    "knn = LinearRegression()\n",
    "knn.fit(X_r_base_train, y_r_base_train)\n",
    "y_pred = knn.predict(X_r_base_test)\n",
    "\n",
    "mae = mean_absolute_error(y_r_base_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_r_base_test, y_pred))\n",
    "r2 = r2_score(y_r_base_test, y_pred)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ffbd0",
   "metadata": {},
   "source": [
    "##### Улучшение бейзлайна для модели регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64dfe3e",
   "metadata": {},
   "source": [
    "Сначала повторим техники из предыдущей ЛР"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db16bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df = pd.read_csv(\"../regression.csv\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "r_df['Date'] = pd.to_datetime(r_df['Date'], dayfirst=True)\n",
    "\n",
    "r_df['Year'] = r_df['Date'].dt.year\n",
    "r_df['Week'] = r_df['Date'].dt.isocalendar().week\n",
    "\n",
    "r_df = r_df.drop(columns=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404aa79",
   "metadata": {},
   "source": [
    "Я пробовал играться с фичёй недели: делал сквозную нумерацию недель в рамках нескольких лет, добавлял синус и косинус недели, но лучшим вариантом в итоге было всё оставить как есть и применить к признаку OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb04b3c",
   "metadata": {},
   "source": [
    "Разобьём датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1bbf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Year']\n",
    "cat_store = ['Store', 'Week']\n",
    "other_feats = ['Holiday_Flag']\n",
    "\n",
    "per_store_count = r_df.groupby('Store').size().iloc[0]\n",
    "k = max(1, int(np.round(0.8 * per_store_count))) \n",
    "store_counts = r_df['Store'].nunique()\n",
    "\n",
    "train = r_df.iloc[: store_counts * k]\n",
    "test = r_df.iloc[store_counts * k :]\n",
    "\n",
    "X_r_train = train.drop(columns=['Weekly_Sales'])\n",
    "X_r_test = test.drop(columns=['Weekly_Sales'])\n",
    "y_r_train = train['Weekly_Sales']\n",
    "y_r_test = test['Weekly_Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8111f",
   "metadata": {},
   "source": [
    "Будем применять OHE для фич с номером магазина и НЕДЕЛИ, т.к. таргет не будет зависеть от них линейно. Нормализуем числовые фичи. Также применим кросс-валидацию и подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94ef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n",
      "Best model: Lasso(alpha=np.float64(2.44205309454865), max_iter=20000, random_state=42)\n",
      "Best params: {'model': Lasso(max_iter=20000, random_state=42), 'model__alpha': np.float64(2.44205309454865)}\n",
      "\n",
      "Test MAE:  70758.16\n",
      "Test RMSE: 109901.27\n",
      "Test R2:   0.9627\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_store),\n",
    "        ('passth', 'passthrough', other_feats)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('model', ElasticNet(max_iter=20000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {'model': [Ridge(max_iter=20000, random_state=42)],\n",
    "     'model__alpha': np.logspace(-4, 2, 50)},\n",
    "    {'model': [Lasso(max_iter=20000, random_state=42)],\n",
    "     'model__alpha': np.logspace(-4, 1, 50)},\n",
    "    {'model': [ElasticNet(max_iter=20000, random_state=42)],\n",
    "     'model__alpha': np.logspace(-4, 1, 50),\n",
    "     'model__l1_ratio': [0.1, 0.5, 0.9]}\n",
    "]\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_r_train, y_r_train)\n",
    "\n",
    "print(\"Best model:\", grid.best_estimator_.named_steps['model'])\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "y_pred = grid.predict(X_r_test)\n",
    "\n",
    "mae  = mean_absolute_error(y_r_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_r_test, y_pred))\n",
    "r2   = r2_score(y_r_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest MAE:  {mae:.2f}\")\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n",
    "print(f\"Test R2:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a007b4",
   "metadata": {},
   "source": [
    "Мы, опять же, сильно улучшили результат. Он даже по большинству метрик лучше чем у KNN\n",
    "\n",
    "| Метрика | Улучшенная модель KNN | Бейзлайн линейной регрессии | Улучшенная линейная рергессия |\n",
    "|-|-|-|-|\n",
    "| MAE | 68212 | 432655 | 70758 |\n",
    "| RMSE | 129073 | 518509 | 109901 |\n",
    "| R2 | 0.949 | 0.129 | 0.963 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93da0c",
   "metadata": {},
   "source": [
    "##### Имплементация логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4fef994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    \"\"\"\n",
    "    Многоклассовая логистическая регрессия с поддержкой L1/L2 регуляризации.\n",
    "    Поддерживаются два метода оптимизации: 'lbfgs' и 'saga'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=1.0, penalty='l2', solver='lbfgs', max_iter=1000, tol=1e-4):\n",
    "        # Инициализация гиперпараметров\n",
    "        self.C = C\n",
    "        self.penalty = penalty\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.classes_ = None  # уникальные классы целевой переменной\n",
    "        self.weights = None   # матрица весов модели\n",
    "\n",
    "    def _one_hot(self, y):\n",
    "        # Преобразование меток в one-hot представление\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "        return encoder.fit_transform(y)\n",
    "\n",
    "    def _loss_and_grad(self, W, X, Y_onehot):\n",
    "        \"\"\"\n",
    "        Вычисление функции потерь (log-loss) и градиента с регуляризацией.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = Y_onehot.shape[1]\n",
    "        W = W.reshape(n_classes, -1)\n",
    "        scores = X.dot(W.T)\n",
    "        probs = softmax(scores)\n",
    "        loss = log_loss(Y_onehot, probs, normalize=True)\n",
    "\n",
    "        # Регуляризация\n",
    "        if self.penalty == 'l2':\n",
    "            loss += 0.5 / (self.C * n_samples) * np.sum(W**2)\n",
    "            grad = (probs - Y_onehot).T.dot(X) / n_samples + W / (self.C * n_samples)\n",
    "        elif self.penalty == 'l1':\n",
    "            loss += 1.0 / (self.C * n_samples) * np.sum(np.abs(W))\n",
    "            grad = (probs - Y_onehot).T.dot(X) / n_samples + np.sign(W) / (self.C * n_samples)\n",
    "\n",
    "        return loss, grad.ravel()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучение модели с выбранным solver'ом.\n",
    "        L-BFGS использует scipy.optimize.minimize,\n",
    "        SAGA реализован через градиентный спуск с адаптивным шагом.\n",
    "        \"\"\"\n",
    "        X = np.array(X, dtype=float)\n",
    "        y = np.array(y)\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(self.classes_)\n",
    "\n",
    "        Y_onehot = self._one_hot(y)\n",
    "        W_init = np.zeros((n_classes, n_features))\n",
    "\n",
    "        if self.solver == 'lbfgs':\n",
    "            # Оптимизация с использованием L-BFGS\n",
    "            res = minimize(\n",
    "                fun=lambda W: self._loss_and_grad(W, X, Y_onehot)[0],\n",
    "                x0=W_init.ravel(),\n",
    "                jac=lambda W: self._loss_and_grad(W, X, Y_onehot)[1],\n",
    "                method='L-BFGS-B',\n",
    "                options={'maxiter': self.max_iter, 'disp': False}\n",
    "            )\n",
    "            self.weights = res.x.reshape(n_classes, n_features)\n",
    "\n",
    "        elif self.solver == 'saga':\n",
    "            # Стохастический градиентный спуск с адаптивным шагом\n",
    "            W = W_init.copy()\n",
    "            lr = 1.0 / (np.max(np.sum(X ** 2, axis=1)) + 1.0)\n",
    "            for _ in range(self.max_iter):\n",
    "                _, grad = self._loss_and_grad(W, X, Y_onehot)\n",
    "                W -= lr * grad.reshape(n_classes, n_features)\n",
    "                if np.linalg.norm(lr * grad) < self.tol:\n",
    "                    break\n",
    "            self.weights = W\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"solver must be 'lbfgs' or 'saga'\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Вычисление вероятностей классов через softmax\n",
    "        X = np.array(X, dtype=float)\n",
    "        scores = X.dot(self.weights.T)\n",
    "        return softmax(scores)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Предсказание классов как argmax вероятностей\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probs, axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddc633",
   "metadata": {},
   "source": [
    "Обучим на данных бейзлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "11472339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_343466/768891952.py:61: DeprecationWarning: scipy.optimize: The `disp` and `iprint` options of the L-BFGS-B solver are deprecated and will be removed in SciPy 1.18.0.\n",
      "  res = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3032\n"
     ]
    }
   ],
   "source": [
    "model = MyLogisticRegression()\n",
    "\n",
    "model.fit(X_c_base_train, y_c_base_train)\n",
    "\n",
    "y_pred = model.predict(X_c_base_test)\n",
    "\n",
    "acc = accuracy_score(y_c_base_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82432488",
   "metadata": {},
   "source": [
    "Обучим на данных улучшенной модели. Сразу будем использовать параметры, полученные при подборе гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6c994466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.02      0.05      1000\n",
      "           1       0.45      0.55      0.49      1000\n",
      "           2       0.44      0.20      0.28      1000\n",
      "           3       0.35      0.95      0.51      1000\n",
      "           4       0.63      0.13      0.21      1000\n",
      "           5       0.52      0.34      0.41      1000\n",
      "           6       0.36      0.69      0.47      1000\n",
      "           7       0.40      0.20      0.27      1000\n",
      "           8       0.37      0.29      0.32      1000\n",
      "           9       0.43      0.66      0.52      1000\n",
      "\n",
      "    accuracy                           0.40     10000\n",
      "   macro avg       0.46      0.40      0.35     10000\n",
      "weighted avg       0.46      0.40      0.35     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('float_scaling', StandardScaler(), float_features),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('logreg', MyLogisticRegression(penalty='l1', C=0.1, solver='saga'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_c_train, y_c_train)\n",
    "preds = pipeline.predict(X_c_test)\n",
    "acc = accuracy_score(y_c_test, preds)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "print(classification_report(y_c_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d384f4",
   "metadata": {},
   "source": [
    "Получилось хуже, чем у sklearn. Многое можно улучшить"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14820d2c",
   "metadata": {},
   "source": [
    "#####  Имплементация линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(BaseEstimator, RegressorMixin):\n",
    "\n",
    "    def __init__(self, fit_intercept: bool = True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучение модели\n",
    "        \"\"\"\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Добавляем столбец единиц для intercept при необходимости\n",
    "        if self.fit_intercept:\n",
    "            X_design = np.c_[np.ones(X.shape[0]), X]\n",
    "        else:\n",
    "            X_design = X\n",
    "\n",
    "        # МНК\n",
    "        coef, _, _, _ = np.linalg.lstsq(X_design, y, rcond=None)\n",
    "\n",
    "        # Сохраняем коэффициенты в формате sklearn\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = coef[0]\n",
    "            self.coef_ = coef[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = coef\n",
    "\n",
    "        return self  \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказание\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # y = Xw + b\n",
    "        return X @ self.coef_ + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1eab50",
   "metadata": {},
   "source": [
    "Обучим на данных бейзлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "024d132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 423673.5471084352\n",
      "RMSE: 520176.7045925049\n",
      "R2: 0.16364456646517678\n"
     ]
    }
   ],
   "source": [
    "knn = MyLinearRegression()\n",
    "knn.fit(X_r_base_train, y_r_base_train)\n",
    "y_pred = knn.predict(X_r_base_test)\n",
    "\n",
    "mae = mean_absolute_error(y_r_base_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_r_base_test, y_pred))\n",
    "r2 = r2_score(y_r_base_test, y_pred)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d53aef",
   "metadata": {},
   "source": [
    "Обучим на данных улучшенной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32168510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: MyLinearRegression()\n",
      "Best params: {'model__fit_intercept': True}\n",
      "\n",
      "Test MAE:  70854.16\n",
      "Test RMSE: 109924.95\n",
      "Test R2:   0.9627\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_store),\n",
    "        ('passth', 'passthrough', other_feats)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('model', MyLinearRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_r_train, y_r_train)\n",
    "\n",
    "print(\"Best model:\", grid.best_estimator_.named_steps['model'])\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "y_pred = grid.predict(X_r_test)\n",
    "\n",
    "mae  = mean_absolute_error(y_r_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_r_test, y_pred))\n",
    "r2   = r2_score(y_r_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest MAE:  {mae:.2f}\")\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n",
    "print(f\"Test R2:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c18be0",
   "metadata": {},
   "source": [
    "Получилось даже неплохо\n",
    "\n",
    "| Метрика | Бейзлайн линейной регрессии | Бейзлайн имплементации | Улучшенная линейная рергессия | Улучшенная имплементация |\n",
    "|-|-|-|-|-|\n",
    "| MAE | 432655 | 423674 | 70758 | 70854 |\n",
    "| RMSE | 518509 | 520177 | 109901 | 109925 |\n",
    "| R2 | 0.129 | 0.164 | 0.963 | 0.963 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
