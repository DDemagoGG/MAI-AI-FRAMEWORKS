{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a74c08b",
   "metadata": {},
   "source": [
    "# Лабораторная работа №3 \"Проведение исследований с решающим деревом\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199765a",
   "metadata": {},
   "source": [
    "### Ход работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6eb79",
   "metadata": {},
   "source": [
    "Импортируем библиотеки перед работой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4722fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from collections import Counter\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73d79a",
   "metadata": {},
   "source": [
    "##### Создание бейзлайна для модели классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ab95d",
   "metadata": {},
   "source": [
    "Проведём те же манипуляции, что и ранне: выгрузим датасет и минимально его обработаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68d5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_base_df = pd.read_csv(\"../classification.csv\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "c_base_df = c_base_df.drop(columns=['instance_id', 'obtained_date', 'track_name', 'artist_name'])\n",
    "\n",
    "c_base_df.drop_duplicates()\n",
    "\n",
    "c_base_df['tempo'] = pd.to_numeric(c_base_df['tempo'], errors='coerce')\n",
    "\n",
    "le = LabelEncoder()\n",
    "c_base_df['mode'] = le.fit_transform(c_base_df['mode'])\n",
    "c_base_df['music_genre'] = le.fit_transform(c_base_df['music_genre'])\n",
    "c_base_df['key'] = le.fit_transform(c_base_df['key'])\n",
    "\n",
    "median_tempo = c_base_df['tempo'].median()\n",
    "c_base_df['tempo'] = c_base_df['tempo'].fillna(median_tempo)\n",
    "\n",
    "X_c_base = c_base_df.drop(columns=[\"music_genre\"])\n",
    "y_c_base = c_base_df[\"music_genre\"]\n",
    "\n",
    "X_c_base_train, X_c_base_test, y_c_base_train, y_c_base_test = train_test_split(\n",
    "    X_c_base,\n",
    "    y_c_base,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_c_base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af234879",
   "metadata": {},
   "source": [
    "Теперь обучим модель из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e88a514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4298\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_c_base_train, y_c_base_train)\n",
    "\n",
    "y_c_base_pred = model.predict(X_c_base_test)\n",
    "\n",
    "accuracy = accuracy_score(y_c_base_test, y_c_base_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e076807",
   "metadata": {},
   "source": [
    "##### Улучшение бейзлайна для модели классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "073c0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.27      0.27      1000\n",
      "           1       0.65      0.66      0.66      1000\n",
      "           2       0.39      0.41      0.40      1000\n",
      "           3       0.74      0.74      0.74      1000\n",
      "           4       0.43      0.43      0.43      1000\n",
      "           5       0.46      0.47      0.46      1000\n",
      "           6       0.31      0.34      0.32      1000\n",
      "           7       0.38      0.37      0.38      1000\n",
      "           8       0.25      0.23      0.24      1000\n",
      "           9       0.41      0.38      0.39      1000\n",
      "\n",
      "    accuracy                           0.43     10000\n",
      "   macro avg       0.43      0.43      0.43     10000\n",
      "weighted avg       0.43      0.43      0.43     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_c_base_test, y_c_base_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a9cd6",
   "metadata": {},
   "source": [
    "Это лучше чем регрессия. Теперь преобразуем датасет как ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3356132",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = pd.read_csv(\"../classification.csv\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "c_df = c_df.drop(columns=['instance_id', 'obtained_date', 'track_name', 'artist_name'])\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "scaled = sc.fit_transform(c_df[['loudness', 'acousticness', 'energy']]) # ранее я не замечал, что loudness просто съедала остальные фичи \n",
    "pca = PCA(n_components=2)\n",
    "c_df[['pc1', 'pc2']] = pca.fit_transform(scaled)\n",
    "c_df = c_df.drop(columns=['loudness', 'acousticness', 'energy'])\n",
    "\n",
    "c_df['tempo'] = pd.to_numeric(c_df['tempo'], errors='coerce')\n",
    "\n",
    "le = LabelEncoder()\n",
    "c_df['music_genre'] = le.fit_transform(c_df['music_genre'])\n",
    "c_df['mode'] = le.fit_transform(c_df['mode'])\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_key = ohe.fit_transform(c_df[['key']])\n",
    "encoded_df_key = pd.DataFrame(encoded_key, columns=ohe.get_feature_names_out(['key']))\n",
    "c_df = c_df.drop(columns=['key']).reset_index(drop=True)\n",
    "c_df = pd.concat([c_df, encoded_df_key], axis=1)\n",
    "\n",
    "c_df['duration_ms'] = c_df['duration_ms'].replace(-1, np.nan)\n",
    "\n",
    "c_df['instrumental_flag'] = (c_df['instrumentalness'] > 0.05).astype(int)\n",
    "c_df = c_df.drop(columns=['instrumentalness'])\n",
    "\n",
    "c_df['undefined_tempo'] = c_df['tempo'].isna().astype(int)\n",
    "\n",
    "median_tempo = c_df['tempo'].median()\n",
    "c_df['tempo'] = c_df['tempo'].fillna(median_tempo)\n",
    "\n",
    "median_duration = c_df['duration_ms'].median()\n",
    "c_df['duration_ms'] = c_df['duration_ms'].fillna(median_duration)\n",
    "\n",
    "float_features = [\n",
    "    'popularity', 'danceability', 'duration_ms',\n",
    "    'liveness', 'speechiness', 'tempo',\n",
    "    'valence', 'pc1', 'pc2'\n",
    "]\n",
    "\n",
    "other_features = [\n",
    "    'mode', 'instrumental_flag', 'undefined_tempo'\n",
    "] + list(encoded_df_key.columns) \n",
    "\n",
    "X_c = c_df[float_features + other_features]\n",
    "y_c = c_df['music_genre']\n",
    "\n",
    "X_c_train, X_c_test, y_c_train, y_c_test = train_test_split(\n",
    "    X_c, y_c, test_size=0.2, stratify=y_c, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5bb3a",
   "metadata": {},
   "source": [
    "Перейдём к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705344c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best params: {'dt__class_weight': 'balanced', 'dt__criterion': 'entropy', 'dt__max_depth': 12, 'dt__max_features': None, 'dt__min_samples_leaf': 37, 'dt__min_samples_split': 33}\n",
      "Score: 0.5141\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.29      0.32      1000\n",
      "           1       0.69      0.66      0.68      1000\n",
      "           2       0.50      0.40      0.44      1000\n",
      "           3       0.82      0.79      0.81      1000\n",
      "           4       0.48      0.50      0.49      1000\n",
      "           5       0.53      0.48      0.51      1000\n",
      "           6       0.42      0.43      0.42      1000\n",
      "           7       0.47      0.46      0.47      1000\n",
      "           8       0.41      0.44      0.42      1000\n",
      "           9       0.47      0.69      0.56      1000\n",
      "\n",
      "    accuracy                           0.51     10000\n",
      "   macro avg       0.52      0.51      0.51     10000\n",
      "weighted avg       0.52      0.51      0.51     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('dt', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'dt__criterion': ['gini', 'entropy'],\n",
    "    'dt__max_depth': [None, 5, 8, 12, 16, 20, 30],\n",
    "    'dt__min_samples_split': randint(2, 60),\n",
    "    'dt__min_samples_leaf': randint(1, 40),\n",
    "    'dt__max_features': [None, 'sqrt', 'log2', 'auto'],\n",
    "    'dt__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=60,                  \n",
    "    scoring='accuracy',         \n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "rand_search.fit(X_c_train, y_c_train)\n",
    "\n",
    "print(\"Best params:\", rand_search.best_params_)\n",
    "best_model = rand_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_c_test)\n",
    "\n",
    "print(\"Score:\", accuracy_score(y_c_test, y_pred))\n",
    "\n",
    "print(classification_report(y_c_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae191aa",
   "metadata": {},
   "source": [
    "##### Создание бейзлайна для модели регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a716a65",
   "metadata": {},
   "source": [
    "Сделаем всё то же, что и ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5b00ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_base_df = pd.read_csv(\"../regression.csv\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "r_base_df['Date'] = pd.to_datetime(r_base_df['Date'], dayfirst=True)\n",
    "\n",
    "r_base_df[\"Year\"] = r_base_df[\"Date\"].dt.year\n",
    "r_base_df[\"Month\"] = r_base_df[\"Date\"].dt.month\n",
    "r_base_df[\"Day\"] = r_base_df[\"Date\"].dt.day\n",
    "\n",
    "r_base_df = r_base_df.drop(columns=['Date'])\n",
    "\n",
    "per_store_count = r_base_df.groupby('Store').size().iloc[0]\n",
    "k = max(1, int(np.round(0.8 * per_store_count))) \n",
    "store_counts = r_base_df['Store'].nunique()\n",
    "\n",
    "train = r_base_df.iloc[: store_counts * k]\n",
    "test = r_base_df.iloc[store_counts * k :]\n",
    "\n",
    "X_r_base_train = train.drop(columns=['Weekly_Sales'])\n",
    "X_r_base_test = test.drop(columns=['Weekly_Sales'])\n",
    "y_r_base_train = train['Weekly_Sales']\n",
    "y_r_base_test = test['Weekly_Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2c533",
   "metadata": {},
   "source": [
    "И теперь обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db28f8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 330834.6335131109\n",
      "RMSE: 433606.2436003742\n",
      "R2: 0.41886084325173134\n"
     ]
    }
   ],
   "source": [
    "dt_reg = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "dt_reg.fit(X_r_base_train, y_r_base_train)\n",
    "\n",
    "y_test_pred  = dt_reg.predict(X_r_base_test)\n",
    "\n",
    "mae = mean_absolute_error(y_r_base_test, y_test_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_r_base_test, y_test_pred))\n",
    "r2 = r2_score(y_r_base_test, y_test_pred)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa000476",
   "metadata": {},
   "source": [
    "##### Улучшение бейзлайна для модели регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5cc74",
   "metadata": {},
   "source": [
    "Сначала повторим техники из предыдущей ЛР"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1fbd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df = pd.read_csv(\"../regression.csv\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "r_df['Date'] = pd.to_datetime(r_df['Date'], dayfirst=True)\n",
    "\n",
    "r_df['Year'] = r_df['Date'].dt.year\n",
    "r_df['Week'] = r_df['Date'].dt.isocalendar().week\n",
    "\n",
    "r_df = r_df.drop(columns=['Date'])\n",
    "\n",
    "cat_store = ['Store', 'Week']\n",
    "other_feats = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Year', 'Holiday_Flag']\n",
    "\n",
    "per_store_count = r_df.groupby('Store').size().iloc[0]\n",
    "k = max(1, int(np.round(0.8 * per_store_count))) \n",
    "store_counts = r_df['Store'].nunique()\n",
    "\n",
    "train = r_df.iloc[: store_counts * k]\n",
    "test = r_df.iloc[store_counts * k :]\n",
    "\n",
    "X_r_train = train.drop(columns=['Weekly_Sales'])\n",
    "X_r_test = test.drop(columns=['Weekly_Sales'])\n",
    "y_r_train = train['Weekly_Sales']\n",
    "y_r_test = test['Weekly_Sales']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_store),\n",
    "        ('passth', 'passthrough', other_feats)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74768b43",
   "metadata": {},
   "source": [
    "Перейдём к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b37b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'dt__criterion': 'poisson', 'dt__max_depth': None, 'dt__max_features': None, 'dt__min_samples_leaf': 1, 'dt__min_samples_split': 3}\n",
      "MAE: 68922.4129770115\n",
      "RMSE: 132851.9622745847\n",
      "R2: 0.9454461969234228\n"
     ]
    }
   ],
   "source": [
    "dt_pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('dt', DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'dt__max_depth': [None, 3, 4, 5, 6],\n",
    "    'dt__min_samples_split': randint(2, 10),\n",
    "    'dt__min_samples_leaf': randint(1, 10),\n",
    "    'dt__max_features': [None, 'sqrt', 'log2'],\n",
    "    'dt__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=dt_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  \n",
    "    cv=kf,\n",
    "    scoring='neg_mean_squared_error', \n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "rand_search.fit(X_r_train, y_r_train)\n",
    "\n",
    "print(\"Best params:\", rand_search.best_params_)\n",
    "best_model = rand_search.best_estimator_\n",
    "y_test_pred  = best_model.predict(X_r_test)\n",
    "\n",
    "mae = mean_absolute_error(y_r_test, y_test_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_r_test, y_test_pred))\n",
    "r2 = r2_score(y_r_test, y_test_pred)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab896f4",
   "metadata": {},
   "source": [
    "##### Имплементация базового класса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb3f2d",
   "metadata": {},
   "source": [
    "класс ноды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23900aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    Узел дерева. Минимальный набор полей для экономии памяти.\n",
    "    \"\"\"\n",
    "    __slots__ = (\"feature\", \"threshold\", \"left\", \"right\", \"value\")\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature = None      # индекс признака для сплита\n",
    "        self.threshold = None    # порог для сплита\n",
    "        self.left = None         # левый потомок\n",
    "        self.right = None        # правый потомок\n",
    "        self.value = None        # значение в листе (предсказание)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf55050",
   "metadata": {},
   "source": [
    "сам базовый класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDecisionTree(ABC):\n",
    "    \"\"\"\n",
    "    Базовый класс дерева (для классификации и регрессии).\n",
    "    Поддерживает criterion='entropy' или 'poisson'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        criterion=\"entropy\",\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=None\n",
    "    ):\n",
    "        if criterion not in (\"entropy\", \"poisson\"):\n",
    "            raise ValueError(\"Поддерживаются только criterion='entropy' или 'poisson'\")\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.root = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def _calculate_impurity(self, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Вычисляет \"нечистоту\" узла.\n",
    "        Для классификации — энтропия.\n",
    "        Для Poisson-регрессии — Poisson deviance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _leaf_value(self, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Значение в листе.\n",
    "        Для классификации — наиболее частый класс.\n",
    "        Для Poisson-регрессии — среднее.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _best_split(self, X, y, sample_weight):\n",
    "        \"\"\"\n",
    "        Ищет оптимальный сплит по всем (или случайным) признакам,\n",
    "        который максимизирует прирост качества (gain).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # выбираем признаки для рассмотрения (все или подвыборка)\n",
    "        features = np.arange(n_features)\n",
    "        if self.max_features is not None:\n",
    "            size = min(self.max_features, n_features)\n",
    "            features = np.random.choice(features, size, replace=False)\n",
    "\n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        base_impurity = self._calculate_impurity(y, sample_weight)\n",
    "\n",
    "        # перебираем признаки и пороги для разделения\n",
    "        for feature in features:\n",
    "            values = X[:, feature]\n",
    "            thresholds = np.unique(values)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                left_mask = values <= thr\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                # проверяем минимальное количество объектов в листе\n",
    "                if (\n",
    "                    left_mask.sum() < self.min_samples_leaf\n",
    "                    or right_mask.sum() < self.min_samples_leaf\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                y_left, y_right = y[left_mask], y[right_mask]\n",
    "                w_left = sample_weight[left_mask]\n",
    "                w_right = sample_weight[right_mask]\n",
    "\n",
    "                w_total = np.sum(sample_weight)\n",
    "                w_l = np.sum(w_left)\n",
    "                w_r = np.sum(w_right)\n",
    "\n",
    "                # вычисляем прирост качества (information gain)\n",
    "                impurity_left = self._calculate_impurity(y_left, w_left)\n",
    "                impurity_right = self._calculate_impurity(y_right, w_right)\n",
    "\n",
    "                gain = base_impurity - (w_l / w_total) * impurity_left - (w_r / w_total) * impurity_right\n",
    "\n",
    "                # сохраняем лучший сплит\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = thr\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "\n",
    "    def _build_tree(self, X, y, sample_weight, depth):\n",
    "        \"\"\"\n",
    "        Рекурсивная постройка дерева.\n",
    "        \"\"\"\n",
    "        node = TreeNode()  # создаем узел\n",
    "        node.value = self._leaf_value(y, sample_weight)  # значение узла (листа)\n",
    "\n",
    "        # проверка условий остановки\n",
    "        if (\n",
    "            len(y) < self.min_samples_split\n",
    "            or (self.max_depth is not None and depth >= self.max_depth)\n",
    "            or len(set(y)) == 1 \n",
    "        ):\n",
    "            return node  \n",
    "\n",
    "        # ищем лучший сплит\n",
    "        feature, threshold, gain = self._best_split(X, y, sample_weight)\n",
    "        if feature is None or gain <= 0:\n",
    "            return node  \n",
    "\n",
    "        node.feature = feature\n",
    "        node.threshold = threshold\n",
    "\n",
    "        # рекурсивно строим левое и правое поддеревья\n",
    "        mask = X[:, feature] <= threshold\n",
    "        node.left = self._build_tree(X[mask], y[mask], sample_weight[mask], depth + 1)\n",
    "        node.right = self._build_tree(X[~mask], y[~mask], sample_weight[~mask], depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Обучение дерева на данных X, y с возможностью учета весов объектов.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        self.n_features_ = X.shape[1]\n",
    "\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(len(y))\n",
    "        else:\n",
    "            sample_weight = np.asarray(sample_weight)\n",
    "\n",
    "        # создаем дерево начиная с корня\n",
    "        self.root = self._build_tree(X, y, sample_weight, depth=0)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        \"\"\"\n",
    "        Проход по дереву для одного объекта x.\n",
    "        \"\"\"\n",
    "        while node.feature is not None:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value  \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказание значений для массива объектов X.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d846811",
   "metadata": {},
   "source": [
    "##### Классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feeed759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier(BaseDecisionTree):\n",
    "    \"\"\"\n",
    "    Реализация решающего дерева для классификации на основе BaseDecisionTree.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        criterion=\"entropy\",\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=None,\n",
    "        class_weight=None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            criterion=criterion,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features\n",
    "        )\n",
    "        if criterion != \"entropy\":\n",
    "            raise ValueError(\"Поддерживается только criterion='entropy'\")\n",
    "        self.criterion = criterion\n",
    "        self.class_weight = class_weight\n",
    "        self.classes_ = None\n",
    "        self.class_weights_ = None\n",
    "\n",
    "    def _compute_class_weights(self, y):\n",
    "        \"\"\"\n",
    "        class_weight='balanced'\n",
    "        \"\"\"\n",
    "        counts = Counter(y)\n",
    "        n_samples = len(y)\n",
    "        n_classes = len(counts)\n",
    "        weights = {cls: n_samples / (n_classes * cnt) for cls, cnt in counts.items()}\n",
    "        return weights\n",
    "\n",
    "    def _calculate_impurity(self, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Энтропия с учётом весов объектов.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if sample_weight is None:\n",
    "            counts = np.bincount(y)\n",
    "            probs = counts[counts > 0] / len(y)\n",
    "        else:\n",
    "            total_weight = np.sum(sample_weight)\n",
    "            weights_per_class = {}\n",
    "            for label, w in zip(y, sample_weight):\n",
    "                weights_per_class[label] = weights_per_class.get(label, 0) + w\n",
    "            probs = np.array(list(weights_per_class.values())) / total_weight\n",
    "\n",
    "        return -np.sum(probs * np.log2(probs + 1e-9))\n",
    "\n",
    "    def _leaf_value(self, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Возвращает класс с наибольшим суммарным весом (или частотой).\n",
    "        \"\"\"\n",
    "        if sample_weight is None:\n",
    "            counts = Counter(y)\n",
    "        else:\n",
    "            counts = {}\n",
    "            for label, w in zip(y, sample_weight):\n",
    "                counts[label] = counts.get(label, 0) + w\n",
    "        return max(counts, key=counts.get)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        if self.class_weight == \"balanced\":\n",
    "            self.class_weights_ = self._compute_class_weights(y)\n",
    "            sample_weight = np.array([self.class_weights_[cls] for cls in y])\n",
    "        else:\n",
    "            sample_weight = np.ones(len(y))\n",
    "\n",
    "        return super().fit(X, y, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45d336",
   "metadata": {},
   "source": [
    "Обучим на данных бейзлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e474c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3827\n"
     ]
    }
   ],
   "source": [
    "model = MyDecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "model.fit(X_c_base_train, y_c_base_train)\n",
    "\n",
    "y_c_base_pred = model.predict(X_c_base_test)\n",
    "\n",
    "accuracy = accuracy_score(y_c_base_test, y_c_base_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdd72b",
   "metadata": {},
   "source": [
    "(я забыл вывести отчёт, а колаб уже всё затёр. Эта штучка обучалась около часа, поэтому да...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9253b9",
   "metadata": {},
   "source": [
    "Обучим на данных улучшенной модели. Сразу будем использовать параметры, полученные при подборе гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0dbb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.28      0.32      1000\n",
      "           1       0.70      0.65      0.67      1000\n",
      "           2       0.50      0.40      0.45      1000\n",
      "           3       0.82      0.79      0.81      1000\n",
      "           4       0.48      0.51      0.49      1000\n",
      "           5       0.53      0.48      0.51      1000\n",
      "           6       0.42      0.42      0.42      1000\n",
      "           7       0.47      0.47      0.47      1000\n",
      "           8       0.41      0.45      0.43      1000\n",
      "           9       0.47      0.69      0.56      1000\n",
      "\n",
      "    accuracy                           0.52     10000\n",
      "   macro avg       0.52      0.52      0.51     10000\n",
      "weighted avg       0.52      0.52      0.51     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('dt', MyDecisionTreeClassifier(max_depth=12, min_samples_leaf=37, min_samples_split=3))\n",
    "])\n",
    "\n",
    "pipe.fit(X_c_train, y_c_train)\n",
    "\n",
    "\n",
    "y_pred = pipe.predict(X_c_test)\n",
    "\n",
    "print(\"Score:\", accuracy_score(y_c_test, y_pred))\n",
    "\n",
    "print(classification_report(y_c_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2efa76a",
   "metadata": {},
   "source": [
    "Имеем скоры:\n",
    "\n",
    "**Бейзлайн sklearn** - 0.4298\n",
    "\n",
    "**Улучшенная модель sklearn** - 0.5141\n",
    "\n",
    "**Мой бейзлайн** - 0.3827\n",
    "\n",
    "**Мой улучшенный бейзлайн** - 0.5154\n",
    "\n",
    "Моя модель несильно отличается от модели sklearn по скору"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2479e7",
   "metadata": {},
   "source": [
    "##### Регрессор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bd3d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreerRegressor(BaseDecisionTree):\n",
    "    \"\"\"\n",
    "    Решающее дерево для регрессии с Poisson loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            criterion=\"poisson\",\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features\n",
    "        )\n",
    "\n",
    "    def _calculate_impurity(self, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Poisson deviance: sum(y * log(y / mean) - (y - mean))\n",
    "        Для узла: используем среднее по y как baseline.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(y, dtype=float)\n",
    "        else:\n",
    "            w = sample_weight\n",
    "\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        mean_y = np.average(y, weights=w)\n",
    "        mean_y = max(mean_y, 1e-9)  # чтобы избежать log(0)\n",
    "\n",
    "        # Poisson deviance\n",
    "        deviance = np.sum(w * (y * np.log((y + 1e-9) / mean_y) - (y - mean_y)))\n",
    "        return deviance\n",
    "\n",
    "    def _leaf_value(self, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Значение листа — взвешенное среднее y.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if sample_weight is None:\n",
    "            return np.mean(y)\n",
    "        else:\n",
    "            return np.average(y, weights=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46651a2",
   "metadata": {},
   "source": [
    "Обучим на данных бейзлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "087edc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 384023.38268405484\n",
      "RMSE: 484455.802051078\n",
      "R2: 0.2745668328456484\n"
     ]
    }
   ],
   "source": [
    "dt_reg = MyDecisionTreerRegressor(max_depth=3)\n",
    "dt_reg.fit(X_r_base_train, y_r_base_train)\n",
    "\n",
    "y_test_pred  = dt_reg.predict(X_r_base_test)\n",
    "\n",
    "mae = mean_absolute_error(y_r_base_test, y_test_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_r_base_test, y_test_pred))\n",
    "r2 = r2_score(y_r_base_test, y_test_pred)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51564eb",
   "metadata": {},
   "source": [
    "Обучим на данных улучшенной модели. Сразу будем использовать параметры, полученные при подборе гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ef6611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 100450.66639846744\n",
      "RMSE: 215020.25432678152\n",
      "R2: 0.8570948674402575\n"
     ]
    }
   ],
   "source": [
    "dt_pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('dt', MyDecisionTreerRegressor(min_samples_leaf=1, min_samples_split=3))\n",
    "])\n",
    "\n",
    "dt_pipeline.fit(X_r_train, y_r_train)\n",
    "\n",
    "y_test_pred  = dt_pipeline.predict(X_r_test)\n",
    "\n",
    "mae = mean_absolute_error(y_r_test, y_test_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_r_test, y_test_pred))\n",
    "r2 = r2_score(y_r_test, y_test_pred)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb12689",
   "metadata": {},
   "source": [
    "Получили\n",
    "\n",
    "| Метрика | Бейзлайн линейной регрессии | Бейзлайн имплементации | Улучшенная линейная рергессия | Улучшенная имплементация |\n",
    "|-|-|-|-|-|\n",
    "| MAE | 330835 | 384023 | 68922 | 100451 |\n",
    "| RMSE | 433606 | 484456 | 132852 | 215020 |\n",
    "| R2 | 0.419 | 0.275 | 0.945 | 0.857 |\n",
    "\n",
    "Моя имплементация оказалась слабее"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
